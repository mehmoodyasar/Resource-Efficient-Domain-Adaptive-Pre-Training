{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GitHub_DAPT_Phase_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H61BeQWCDyYH"
      },
      "source": [
        "#import the necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "from keras import models \n",
        "from keras import layers\n",
        "#import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv5N44ethHhn",
        "outputId": "1dca1fcb-78cf-4f3b-8fa8-92b843693185"
      },
      "source": [
        "#Mount Google drive\n",
        "#Run this cell only if your data (npy files of LGG and HGG reside on Google \n",
        "#drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set base_path to the location where the data and results of your project\n",
        "#reside\n",
        "base_path = '/content/gdrive/MyDrive/HPT/'"
      ],
      "metadata": {
        "id": "-O2JO9N9ZHrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image shape set to constant for further use\n",
        "# 240, 240 is the size of a slice in BraTS dataset. Same image/slice\n",
        "# is copied to the 3 channels. We need to have 3 channels because we\n",
        "# are using pre-trained ResNet50 (or its variant)\n",
        "IMG_SHAPE = (240, 240, 3)"
      ],
      "metadata": {
        "id": "5uda6Z3MNCe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing TrainX and TrainY\n",
        "Loading HGG and LGG data stored in .npy files. Creating their labels: 0 for LGG and 1 for HGG. Finally, the data and the corresponding labels will be shuffled randomly."
      ],
      "metadata": {
        "id": "RxPZpGQ4tIoh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un3NfVF6fjF5"
      },
      "source": [
        "HGG_cases = 19496\n",
        "LGG_cases = 4926\n",
        "Total_cases = HGG_cases + LGG_cases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7iTNC9nfjF5"
      },
      "source": [
        "#creating a NumPy array for holding all the training data (TrainX)\n",
        "TrainX = np.zeros((Total_cases, 240 , 240, 3), dtype=np.float16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np_7-P3HfjF5",
        "outputId": "083694c2-37d0-42af-b472-9d91cdbae573"
      },
      "source": [
        "print (TrainX.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Ifip27fqFP"
      },
      "source": [
        "#function to load HGG cases stored in BraTS2020_Tumorous_HGG_T1_f16.npy\n",
        "def read_HGG():\n",
        "  HGG_data_one_channel = np.load(base_path + 'Datasets/BraTS2020/BraTS2020_Tumorous_HGG_T1_f16.npy')\n",
        "  print (HGG_data_one_channel.shape)\n",
        "  print (HGG_data_one_channel.dtype)\n",
        "\n",
        "  for i in range (HGG_data_one_channel.shape[0]):\n",
        "    TrainX[i, :, :, 0] = HGG_data_one_channel[i, :, :]\n",
        "    TrainX[i, :, :, 1] = HGG_data_one_channel[i, :, :]\n",
        "    TrainX[i, :, :, 2] = HGG_data_one_channel[i, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mCDJGOif6oW"
      },
      "source": [
        "#function to load LGG cases stored in BraTS2020_Tumorous_LGG_T1_f16.npy\n",
        "def read_LGG():\n",
        "  LGG_data_one_channel = np.load(base_path + 'Datasets/BraTS2020/BraTS2020_Tumorous_LGG_T1_f16.npy')\n",
        "  print (LGG_data_one_channel.shape)\n",
        "  print (LGG_data_one_channel.dtype)\n",
        "\n",
        "  for i in range (LGG_data_one_channel.shape[0]):\n",
        "    TrainX[i + HGG_cases, :, :, 0] = LGG_data_one_channel[i, :, :]\n",
        "    TrainX[i + HGG_cases, :, :, 1] = LGG_data_one_channel[i, :, :]\n",
        "    TrainX[i + HGG_cases, :, :, 2] = LGG_data_one_channel[i, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zycSDA2guxG",
        "outputId": "0b47925c-d2da-42a1-c920-ced0f473bb58"
      },
      "source": [
        "#call the function read_HGG() to load HGG data to TrainX\n",
        "read_HGG()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19496, 240, 240)\n",
            "float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nBZ1iIbgxu-",
        "outputId": "d0457faf-8df7-4e99-e967-58a0c6f77683"
      },
      "source": [
        "#call the function read_LGG() to load LGG data to TrainX\n",
        "read_LGG()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4926, 240, 240)\n",
            "float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEJrY5zNEhy9"
      },
      "source": [
        "#function to define labels. i.e. 1 for HGG and 0 LGG cases\n",
        "def define_labels():\n",
        "  HGG_labels = np.ones(shape=(HGG_cases,1), dtype='uint8')\n",
        "  LGG_labels = np.zeros(shape=(LGG_cases,1), dtype='uint8')\n",
        "\n",
        "  return (np.concatenate((HGG_labels, LGG_labels), axis=0))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9eHk6N1EnC2"
      },
      "source": [
        "#Call the function to create labels and store in TrainY \n",
        "TrainY = define_labels()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy5kOufffjF_",
        "outputId": "636fbdab-a7ff-4fda-b340-0d8202db5604"
      },
      "source": [
        "#Printing the shape of TrainX and TrainY\n",
        "print (TrainX.shape)\n",
        "print (TrainY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24422, 240, 240, 3)\n",
            "(24422, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1_EV1wnEpb1"
      },
      "source": [
        "#Shuffle the data in TrainX and TrainY\n",
        "p = np.random.permutation(TrainX.shape[0])\n",
        "TrainX = TrainX[p]\n",
        "TrainY = TrainY[p]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Phase 2 of DAPT (Start for the First Time)\n",
        "Run the following cells only when starting DAPT (phase 2) for a particular \n",
        "strategy for the first time. **DO NOT** run the following cells if you are\n",
        "resuming phase 2 of DAPT after some epochs.\n",
        "\n",
        "#Appropriate number of layers of the architecture will be un-frozen depending upon the strategy"
      ],
      "metadata": {
        "id": "IL4fmcLEsKUY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QbKr0siD8e0"
      },
      "source": [
        "#####################################################\n",
        "#You need to run phase 2 of DAPT for all the strategies\n",
        "#Please use the variable \"Strategy\" and assign it the \n",
        "#name of the strategy for which you are running the \n",
        "#experiment.\n",
        "#####################################################\n",
        "#Set the name of the strategy\n",
        "Strategy = \"DA_TF_F1B\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load appropriate (based on the strategy) phase 1 model which has been trained for 1000 epochs.\n",
        "if Strategy = \"DA_TF_F1B\" or Strategy = \"DA_TF_F2B\":\n",
        "  phase_2_model = models.load_model(base_path + 'DAPT/Checkpoints/' + Strategy + '/phase_1/checkpoint-1000.h5')\n",
        "else:\n",
        "  phase_2_model = models.load_model(base_path + 'DAPT/Checkpoints/Full Architectures/phase_1/checkpoint-1000.h5')"
      ],
      "metadata": {
        "id": "T3gK_hkaxshr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unfreeze appropriate number of layers depending upon the strategy\n",
        "if Strategy = \"DA_TF_F1B\" or Strategy = \"DA_TF_F2B\" or Strategy = \"DA\": \n",
        "  #unfreeze all the layers except  batchnorm layers\n",
        "  for layer in enumerate(phase_2_model.layers):\n",
        "    if \"_bn\" not in layer.name:\n",
        "      layer.trainable = True\n",
        "\n",
        "elif Strategy = \"DA_L1SB\" or Strategy = \"DA_L1SB_PFT\":\n",
        "  un_freeze_from = -13\n",
        "  # un-freeze the last 1 sub-block (11 layers of convolution base)\n",
        "  for layer in phase_2_model.layers[un_freeze_from:]:\n",
        "    if \"_bn\" not in layer.name:\n",
        "      layer.trainable = True\n",
        "\n",
        "elif Strategy = \"DA_L2SB\" or Strategy = \"DA_L2SB_PFT\":\n",
        "  un_freeze_from = -23\n",
        "  # un-freeze the last 2 sub-blocks (21 layers of convolution base)\n",
        "  for layer in phase_2_model.layers[un_freeze_from:]:\n",
        "    if \"_bn\" not in layer.name:\n",
        "      layer.trainable = True"
      ],
      "metadata": {
        "id": "qWEQGHPIzEU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify that appropriate number of layers of the convolution base are frozen\n",
        "for j, layer in enumerate(phase_2_model.layers):\n",
        "  print (j, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "O6DCYk1T2pOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Since we have changed the trainable property of some layers, we need to \n",
        "#re-compile the model\n",
        "phase_2_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
        ")"
      ],
      "metadata": {
        "id": "EjzwJLI62z4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the checkpoint path \n",
        "checkpoint_path = base_path + 'DAPT/Checkpoints/' + Strategy + '/phase_2'\n",
        "\n",
        "#Define callback to save the model after every epoch\n",
        "callbacks = []\n",
        "callbacks.append(ModelCheckpoint(checkpoint_path + '/checkpoint-{epoch}.h5'))"
      ],
      "metadata": {
        "id": "DrfOpJtG2-FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the total epochs for different strategies\n",
        "if Strategy = \"DA_TF_F1B\" or Strategy = \"DA_TF_F2B\" or Strategy = \"DA\" or Strategy = \"DA_L1SB\" or Strategy = \"DA_L2SB\":\n",
        "  total_epochs = 150 #total epochs for full or partial DAPT strategies\n",
        "else:\n",
        "   total_epochs = 100 #total epochs for hybrid DAPT strategies (DA_L1SB_PFT and DA_L2SB_PFT)\n",
        "\n",
        "#Set batch size and initial epoch number\n",
        "batchSize=16\n",
        "initial_epoch_number = 0"
      ],
      "metadata": {
        "id": "nT3YpyUq3dLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start DAPT phase 2\n",
        "history = phase_2_model.fit(TrainX, TrainY,  batch_size=batchSize, \n",
        "                    epochs=total_epochs,\n",
        "                    initial_epoch=initial_epoch_number,\n",
        "                    verbose=2,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "GVDWGAwK4X4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2 of DAPT (Resume Training)\n",
        "Run the following cells only when resuming DAPT (phase 2) for a particular \n",
        "strategy from a particular epoch number. **DO NOT** run the following cells if you are\n",
        "starting phase 2 of DAPT from epoch no. 0.\n",
        "\n",
        "#The most recent checkpoint will be loaded and training will be resumed from where it was interrupted."
      ],
      "metadata": {
        "id": "O7CNl-Il4dns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set the checkpoint path \n",
        "checkpoint_path = base_path + 'DAPT/Checkpoints/' + Strategy + '/phase_2'\n",
        "\n",
        "#Define callback to save the model after every epoch\n",
        "callbacks = []\n",
        "callbacks.append(ModelCheckpoint(checkpoint_path + '/checkpoint-{epoch}.h5'))"
      ],
      "metadata": {
        "id": "3JknScLTwRT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the total epochs for different strategies\n",
        "if Strategy = \"DA_TF_F1B\" or Strategy = \"DA_TF_F2B\" or Strategy = \"DA\" or Strategy = \"DA_L1SB\" or Strategy = \"DA_L2SB\":\n",
        "  total_epochs = 150 #total epochs for full or partial DAPT strategies\n",
        "else:\n",
        "   total_epochs = 100 #total epochs for hybrid DAPT strategies\n",
        "\n",
        "#Set batch size \n",
        "batchSize=16"
      ],
      "metadata": {
        "id": "DadeswyX5LOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the epoch number from where the training will be resumed.\n",
        "initial_epoch_number = 50 \n",
        "\n",
        "#loading the saved checkpoint from where to resume training \n",
        "phase_2_model = models.load_model(checkpoint_path + '/checkpoint-' + str(initial_epoch_number) + '.h5')"
      ],
      "metadata": {
        "id": "tgaSJyY25QZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resume DAPT phase 2 training\n",
        "history = phase_2_model.fit(TrainX, TrainY,  batch_size=batchSize, \n",
        "                    epochs=total_epochs,\n",
        "                    initial_epoch=initial_epoch_number,\n",
        "                    verbose=2,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "YQ_GCY_z5WsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#END OF PHASE 2 DAPT"
      ],
      "metadata": {
        "id": "IFGi4yrG5k3i"
      }
    }
  ]
}