{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GitHub_DAPT_Phase_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Important Note:\n",
        "You need to run DAPT Phase 3 only for the strategies DA_L1SB_PFT and DA_L2SB_PFT."
      ],
      "metadata": {
        "id": "Ifvx_IhrxsV3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H61BeQWCDyYH"
      },
      "source": [
        "#import the necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "from keras import models \n",
        "from keras import layers\n",
        "#import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv5N44ethHhn",
        "outputId": "1dca1fcb-78cf-4f3b-8fa8-92b843693185"
      },
      "source": [
        "#Mount Google drive\n",
        "#Run this cell only if your data (npy files of LGG and HGG reside on Google \n",
        "#drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set base_path to the location where the data and results of your project\n",
        "#reside\n",
        "base_path = '/content/gdrive/MyDrive/HPT/'"
      ],
      "metadata": {
        "id": "-O2JO9N9ZHrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image shape set to constant for further use\n",
        "# 240, 240 is the size of a slice in BraTS dataset. Same image/slice\n",
        "# is copied to the 3 channels. We need to have 3 channels because we\n",
        "# are using pre-trained ResNet50 (or its variant)\n",
        "IMG_SHAPE = (240, 240, 3)"
      ],
      "metadata": {
        "id": "5uda6Z3MNCe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing TrainX and TrainY\n",
        "Loading HGG and LGG data stored in .npy files. Creating their labels: 0 for LGG and 1 for HGG. Finally, the data and the corresponding labels will be shuffled randomly."
      ],
      "metadata": {
        "id": "RxPZpGQ4tIoh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un3NfVF6fjF5"
      },
      "source": [
        "HGG_cases = 19496\n",
        "LGG_cases = 4926\n",
        "Total_cases = HGG_cases + LGG_cases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7iTNC9nfjF5"
      },
      "source": [
        "#creating a NumPy array for holding all the training data (TrainX)\n",
        "TrainX = np.zeros((Total_cases, 240 , 240, 3), dtype=np.float16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np_7-P3HfjF5",
        "outputId": "083694c2-37d0-42af-b472-9d91cdbae573"
      },
      "source": [
        "print (TrainX.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Ifip27fqFP"
      },
      "source": [
        "#function to load HGG cases stored in BraTS2020_Tumorous_HGG_T1_f16.npy\n",
        "def read_HGG():\n",
        "  HGG_data_one_channel = np.load(base_path + 'Datasets/BraTS2020/BraTS2020_Tumorous_HGG_T1_f16.npy')\n",
        "  print (HGG_data_one_channel.shape)\n",
        "  print (HGG_data_one_channel.dtype)\n",
        "\n",
        "  for i in range (HGG_data_one_channel.shape[0]):\n",
        "    TrainX[i, :, :, 0] = HGG_data_one_channel[i, :, :]\n",
        "    TrainX[i, :, :, 1] = HGG_data_one_channel[i, :, :]\n",
        "    TrainX[i, :, :, 2] = HGG_data_one_channel[i, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mCDJGOif6oW"
      },
      "source": [
        "#function to load LGG cases stored in BraTS2020_Tumorous_LGG_T1_f16.npy\n",
        "def read_LGG():\n",
        "  LGG_data_one_channel = np.load(base_path + 'Datasets/BraTS2020/BraTS2020_Tumorous_LGG_T1_f16.npy')\n",
        "  print (LGG_data_one_channel.shape)\n",
        "  print (LGG_data_one_channel.dtype)\n",
        "\n",
        "  for i in range (LGG_data_one_channel.shape[0]):\n",
        "    TrainX[i + HGG_cases, :, :, 0] = LGG_data_one_channel[i, :, :]\n",
        "    TrainX[i + HGG_cases, :, :, 1] = LGG_data_one_channel[i, :, :]\n",
        "    TrainX[i + HGG_cases, :, :, 2] = LGG_data_one_channel[i, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zycSDA2guxG",
        "outputId": "0b47925c-d2da-42a1-c920-ced0f473bb58"
      },
      "source": [
        "#call the function read_HGG() to load HGG data to TrainX\n",
        "read_HGG()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19496, 240, 240)\n",
            "float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nBZ1iIbgxu-",
        "outputId": "d0457faf-8df7-4e99-e967-58a0c6f77683"
      },
      "source": [
        "#call the function read_LGG() to load LGG data to TrainX\n",
        "read_LGG()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4926, 240, 240)\n",
            "float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEJrY5zNEhy9"
      },
      "source": [
        "#function to define labels. i.e. 1 for HGG and 0 LGG cases\n",
        "def define_labels():\n",
        "  HGG_labels = np.ones(shape=(HGG_cases,1), dtype='uint8')\n",
        "  LGG_labels = np.zeros(shape=(LGG_cases,1), dtype='uint8')\n",
        "\n",
        "  return (np.concatenate((HGG_labels, LGG_labels), axis=0))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9eHk6N1EnC2"
      },
      "source": [
        "#Call the function to create labels and store in TrainY \n",
        "TrainY = define_labels()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy5kOufffjF_",
        "outputId": "636fbdab-a7ff-4fda-b340-0d8202db5604"
      },
      "source": [
        "#Printing the shape of TrainX and TrainY\n",
        "print (TrainX.shape)\n",
        "print (TrainY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24422, 240, 240, 3)\n",
            "(24422, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1_EV1wnEpb1"
      },
      "source": [
        "#Shuffle the data in TrainX and TrainY\n",
        "p = np.random.permutation(TrainX.shape[0])\n",
        "TrainX = TrainX[p]\n",
        "TrainY = TrainY[p]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Phase 3 of DAPT (Start for the First Time)\n",
        "Run the following cells only when starting DAPT (phase 3) for a particular \n",
        "strategy for the first time. **DO NOT** run the following cells if you are\n",
        "resuming phase 3 of DAPT after some epochs.\n",
        "\n",
        "#All the layers of the architecture will be un-frozen in phase 3"
      ],
      "metadata": {
        "id": "IL4fmcLEsKUY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QbKr0siD8e0"
      },
      "source": [
        "#Set the name of the strategy\n",
        "Strategy = \"DA_L1SB_PFT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load appropriate (based on the strategy) phase 2 model which has been trained for 100 epochs.\n",
        "phase_3_model = models.load_model(base_path + 'DAPT/Checkpoints/' + Strategy + '/phase_2/checkpoint-100.h5')\n"
      ],
      "metadata": {
        "id": "T3gK_hkaxshr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in enumerate(phase_3_model.layers):\n",
        "    if \"_bn\" not in layer.name:\n",
        "      layer.trainable = True"
      ],
      "metadata": {
        "id": "mxieSmA4917F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify that all layers of the convolution base are frozen\n",
        "for j, layer in enumerate(phase_3_model.layers):\n",
        "  print (j, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "O6DCYk1T2pOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Since we have changed the trainable property of some layers, we need to \n",
        "#re-compile the model\n",
        "phase_3_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
        ")"
      ],
      "metadata": {
        "id": "EjzwJLI62z4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the checkpoint path \n",
        "checkpoint_path = base_path + 'DAPT/Checkpoints/' + Strategy + '/phase_3'\n",
        "\n",
        "#Define callback to save the model after every epoch\n",
        "callbacks = []\n",
        "callbacks.append(ModelCheckpoint(checkpoint_path + '/checkpoint-{epoch}.h5'))"
      ],
      "metadata": {
        "id": "DrfOpJtG2-FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set total epochs, batch size and initial epoch number\n",
        "total_epochs = 50\n",
        "batchSize=16\n",
        "initial_epoch_number = 0"
      ],
      "metadata": {
        "id": "nT3YpyUq3dLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start DAPT phase 2\n",
        "history = phase_3_model.fit(TrainX, TrainY,  batch_size=batchSize, \n",
        "                    epochs=total_epochs,\n",
        "                    initial_epoch=initial_epoch_number,\n",
        "                    verbose=2,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "GVDWGAwK4X4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xlhuCZCf-UQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3 of DAPT (Resume Training)\n",
        "Run the following cells only when resuming DAPT (phase 3) for a particular \n",
        "strategy from a particular epoch number. **DO NOT** run the following cells if you are\n",
        "starting phase 3 of DAPT from epoch no. 0.\n",
        "\n",
        "#The most recent checkpoint will be loaded and training will be resumed from where it was interrupted."
      ],
      "metadata": {
        "id": "O7CNl-Il4dns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set the checkpoint path \n",
        "checkpoint_path = base_path + 'DAPT/Checkpoints/' + Strategy + '/phase_3'\n",
        "\n",
        "#Define callback to save the model after every epoch\n",
        "callbacks = []\n",
        "callbacks.append(ModelCheckpoint(checkpoint_path + '/checkpoint-{epoch}.h5'))"
      ],
      "metadata": {
        "id": "3JknScLTwRT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set total epochs and batch size \n",
        "total_epochs = 50\n",
        "batchSize=16"
      ],
      "metadata": {
        "id": "DadeswyX5LOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the epoch number from where the training will be resumed.\n",
        "initial_epoch_number = 20 \n",
        "\n",
        "#loading the saved checkpoint from where to resume training \n",
        "phase_3_model = models.load_model(checkpoint_path + '/checkpoint-' + str(initial_epoch_number) + '.h5')"
      ],
      "metadata": {
        "id": "tgaSJyY25QZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resume DAPT phase 2 training\n",
        "history = phase_3_model.fit(TrainX, TrainY,  batch_size=batchSize, \n",
        "                    epochs=total_epochs,\n",
        "                    initial_epoch=initial_epoch_number,\n",
        "                    verbose=2,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "YQ_GCY_z5WsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#END OF PHASE 3 DAPT"
      ],
      "metadata": {
        "id": "IFGi4yrG5k3i"
      }
    }
  ]
}